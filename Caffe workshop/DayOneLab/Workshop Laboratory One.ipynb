{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Exercise One - Writing Your Own Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Species of the Iris Flower using the Famous Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages and dataset needed from Python\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names # These tell us what the inputs are, in this case measurements in cm of iris petals and sepals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names # These are the three species of iris flower that we are trying to predict from the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data - training and validation sets and class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack([iris.data,iris.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = data[0:40,:]\n",
    "data_1 = data[50:90,:]\n",
    "data_2 = data[100:140,:]\n",
    "train = np.append(data_0,data_1,axis=0)\n",
    "train = np.append(train,data_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0_test = data[40:50,:]\n",
    "data_1_test = data[90:100,:]\n",
    "data_2_test = data[140:150,:]\n",
    "test = np.append(data_0_test,data_1_test,axis=0)\n",
    "test = np.append(test,data_2_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitialiseNet(N_Neurons,Input,N_Outputs,Type):\n",
    "    if Type == \"Gaussian\":\n",
    "        W1 = np.random.normal(0,1,size=(N_Neurons, Input.shape[1] + 1))\n",
    "        W2 = np.random.normal(0,1,size=(N_Outputs, N_Neurons + 1))\n",
    "    elif Type == \"Xavier\":\n",
    "        W1 = np.random.uniform(-np.sqrt(6/(Input.shape[1] + N_Neurons)),np.sqrt(6/(Input.shape[1] + N_Neurons)),size=(N_Neurons, Input.shape[1] + 1))\n",
    "        W2 = np.random.uniform(-np.sqrt(6/(Input.shape[1] + N_Neurons)),np.sqrt(6/(Input.shape[1] + N_Neurons)),size=(N_Outputs, N_Neurons + 1))\n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(lab):\n",
    "    y = lab.astype(int)\n",
    "    y_values = np.max(y) + 1\n",
    "    y_hot = np.eye(y_values)[y]\n",
    "    return y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def Sigmoid(X):\n",
    "        X = 1/(1 + np.exp(-X))\n",
    "        return X\n",
    "    \n",
    "    def Tanh(X):\n",
    "        X = np.tanh(X)\n",
    "        return X\n",
    "    \n",
    "    def ReLU(X):\n",
    "        X = X * (X > 0) + 0\n",
    "        return X\n",
    "        \n",
    "    def Softmax(X):\n",
    "        max_x = np.repeat(np.amax(X,axis=1),X.shape[1]).reshape(X.shape[0],X.shape[1])\n",
    "        rowsum_x = np.repeat(np.sum(np.exp(X - max_x), axis=1),X.shape[1]).reshape(X.shape[0],X.shape[1])\n",
    "        X = np.exp(X - max_x) / rowsum_x\n",
    "        return X\n",
    "    \n",
    "    # Uncomment the code below and complete it for the advanced exercise\n",
    "    \n",
    "    '''\n",
    "    def SoftPlus(X):\n",
    "        X =\n",
    "        return X\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronPrime:\n",
    "    def SigmoidPrime(X):\n",
    "        X = Neuron.Sigmoid(X) * (1 - Neuron.Sigmoid(X))\n",
    "        return X\n",
    "    \n",
    "    def TanhPrime(X):\n",
    "        X = 1 - np.tanh(X)**2\n",
    "        return X\n",
    "    \n",
    "    def ReLUPrime(X):\n",
    "        X = X * (X > 0) + 0\n",
    "        X[X>0] = 1\n",
    "        return X\n",
    "    \n",
    "    # Uncomment the code below and complete it for the advanced exercise\n",
    "    \n",
    "    '''\n",
    "    def SoftPlusPrime(X):\n",
    "        X =\n",
    "        return X\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forwards(Input,Theta1,Theta2,Output,Activation,Regularisation):\n",
    "    m = Input.shape[0]\n",
    "    inp = np.column_stack([np.ones(m),Input])\n",
    "    z = np.dot(inp,Theta1.T)\n",
    "    a = np.column_stack([np.ones(m),Activation(z)])\n",
    "    prds = Neuron.Softmax(np.dot(a,Theta2.T))\n",
    "    if Regularisation:\n",
    "        Reg = 0.0005/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "        J = -1/m * np.sum(OneHot(Output)*np.log(prds)) + Reg\n",
    "    else:\n",
    "        J = -1/m * np.sum(OneHot(Output)*np.log(prds))\n",
    "    return inp, z, a, prds, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backwards(I,Z,A,Predicted,Output,Derivative,Regularisation):\n",
    "    m = len(Predicted)\n",
    "    D2 = Predicted - OneHot(Output)\n",
    "    D1 = np.dot(D2,Theta2[:,1:])*Derivative(Z)\n",
    "    Delta_2 = np.dot(D2.T,A)\n",
    "    Delta_1 = np.dot(D1.T,I)\n",
    "    if Regularisation:\n",
    "        Delta_2 = Delta_2/m + np.column_stack([np.zeros(Theta2.shape[0]),(Theta2[:,1:]**2)*(0.0005/m)])\n",
    "        Delta_1 = Delta_1/m + np.column_stack([np.zeros(Theta1.shape[0]),(Theta1[:,1:]**2)*(0.0005/m)])\n",
    "    else:\n",
    "        Delta_2 = Delta_2/m\n",
    "        Delta_1 = Delta_1/m\n",
    "    return Delta_2, Delta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Pred,Labs):\n",
    "    acc = np.sum(Labs == np.argmax(Pred, axis=1)).astype(int) / len(Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericalGradients(x,y,initTh1,initTh2,Activation):\n",
    "    e = 1e-5\n",
    "    perturb_1 = np.zeros((initTh1.shape[0],initTh1.shape[1]))\n",
    "    numgrad_1 = np.zeros((initTh1.shape[0],initTh1.shape[1]))\n",
    "    perturb_2 = np.zeros((initTh2.shape[0],initTh2.shape[1]))\n",
    "    numgrad_2 = np.zeros((initTh2.shape[0],initTh2.shape[1]))\n",
    "    for i in range(initTh1.shape[0]):\n",
    "        for j in range(initTh1.shape[1]):\n",
    "            perturb_1[i,j] = e\n",
    "            lg2 = initTh1 + perturb_1\n",
    "            lg1 = initTh1 - perturb_1\n",
    "            \n",
    "            _,_,_,_, J_lg1 = Forwards(x,lg1,initTh2,y,Activation,Regularisation=False)\n",
    "            \n",
    "            _,_,_,_, J_lg2 = Forwards(x,lg2,initTh2,y,Activation,Regularisation=False)\n",
    "             \n",
    "            numgrad_1[i,j] = (J_lg2 - J_lg1) / (2*e)\n",
    "            perturb_1[i,j] = 0\n",
    "            \n",
    "    for i in range(initTh2.shape[0]):\n",
    "        for j in range(initTh2.shape[1]):\n",
    "            perturb_2[i,j] = e\n",
    "            lg2 = initTh2 + perturb_2\n",
    "            lg1 = initTh2 - perturb_2\n",
    "            \n",
    "            _,_,_,_, J_lg1 = Forwards(x,initTh1,lg1,y,Activation,Regularisation=False)\n",
    "            \n",
    "            _,_,_,_, J_lg2 = Forwards(x,initTh1,lg2,y,Activation,Regularisation=False)\n",
    "            \n",
    "            numgrad_2[i,j] = (J_lg2 - J_lg1) / (2*e)\n",
    "            perturb_2[i,j] = 0\n",
    "    return numgrad_1, numgrad_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved theta weight values \n",
    "Theta1_init = np.load(\"Theta1_ini.npy\")\n",
    "Theta2_init = np.load(\"Theta2_ini.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set theta values to initialise the weights\n",
    "Theta1 = Theta1_init\n",
    "Theta2 = Theta2_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Gradient Checking - are our analytical gradients working as intended?\n",
    "numg1, numg2 = NumericalGradients(train[:,0:4],train[:,4],Theta1_init,Theta2_init,Neuron.Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00214459,  0.01048171,  0.00728765,  0.00274818,  0.00034117],\n",
       "       [ 0.04467811,  0.22744304,  0.15915843,  0.06726992,  0.01300343],\n",
       "       [-0.0024653 , -0.00988403, -0.00894719,  0.00281946,  0.00230698],\n",
       "       [-0.03018869, -0.17118835, -0.10431424, -0.08985582, -0.02771159],\n",
       "       [ 0.01310075,  0.07659247,  0.04478484,  0.04573449,  0.01486346]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19794978, -0.00333724, -0.05371027, -0.19131629, -0.14276425,\n",
       "        -0.19249112],\n",
       "       [ 0.22876753,  0.00205344,  0.08667067,  0.2226982 ,  0.16818173,\n",
       "         0.21427808],\n",
       "       [-0.03081775,  0.0012838 , -0.0329604 , -0.03138191, -0.02541748,\n",
       "        -0.02178695]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One forward pass through the neural network to obtain the analytical gradients using initial theta weights.\n",
    "# NOTE: Make sure that Regularisation is set to False! Otherwise the gradients will never match.\n",
    "inp_numg, z_numg, a_numg, preds_numg, J_numg = Forwards(train[:,0:4],Theta1_init,Theta2_init,train[:,4],Neuron.Sigmoid,Regularisation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass for the gradients\n",
    "d2, d1 = Backwards(inp_numg, z_numg, a_numg, preds_numg,train[:,4],NeuronPrime.SigmoidPrime,Regularisation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00214459,  0.01048171,  0.00728765,  0.00274818,  0.00034117],\n",
       "       [ 0.04467811,  0.22744304,  0.15915843,  0.06726992,  0.01300343],\n",
       "       [-0.0024653 , -0.00988403, -0.00894719,  0.00281946,  0.00230698],\n",
       "       [-0.03018869, -0.17118835, -0.10431424, -0.08985582, -0.02771159],\n",
       "       [ 0.01310075,  0.07659247,  0.04478484,  0.04573449,  0.01486346]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19794978, -0.00333724, -0.05371027, -0.19131629, -0.14276425,\n",
       "        -0.19249112],\n",
       "       [ 0.22876753,  0.00205344,  0.08667067,  0.2226982 ,  0.16818173,\n",
       "         0.21427808],\n",
       "       [-0.03081775,  0.0012838 , -0.0329604 , -0.03138191, -0.02541748,\n",
       "        -0.02178695]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6153832042909638e-11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the ratio of normed differences between Numerical and Analytical Gradients for each set of weights.\n",
    "# The ratio should be less than 1e-7 if our analytical gradients are correct\n",
    "np.linalg.norm(d1-numg1)/np.linalg.norm(d1+numg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1286655876792176e-11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(d2-numg2)/np.linalg.norm(d2+numg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network to Predict Species of Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly sample values to initialise theta weights\n",
    "Theta1_init, Theta2_init = InitialiseNet(5,train[:,0:4],3,\"Xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set theta matrices to initial values\n",
    "Theta1 = Theta1_init\n",
    "Theta2 = Theta2_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 1000 # Numnber of training iterations. Because we are using the whole training set, iterations = epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # The learning rate for Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1  Loss:  3.23464719729  Test Loss:  2.73231282119 Accuracy (%):  0.333333333333\n",
      "Iteration : 100  Loss:  0.361862877309  Test Loss:  0.347991444347 Accuracy (%):  0.8\n",
      "Iteration : 200  Loss:  0.27382764989  Test Loss:  0.241857420213 Accuracy (%):  0.866666666667\n",
      "Iteration : 300  Loss:  0.176516479245  Test Loss:  0.137513372196 Accuracy (%):  0.966666666667\n",
      "Iteration : 400  Loss:  0.136927742682  Test Loss:  0.0912432410881 Accuracy (%):  0.966666666667\n",
      "Iteration : 500  Loss:  0.118888349107  Test Loss:  0.0686860737721 Accuracy (%):  0.966666666667\n",
      "Iteration : 600  Loss:  0.109018941589  Test Loss:  0.0556625731907 Accuracy (%):  1.0\n",
      "Iteration : 700  Loss:  0.10279117196  Test Loss:  0.0471216999512 Accuracy (%):  1.0\n",
      "Iteration : 800  Loss:  0.0982863439378  Test Loss:  0.0408951286172 Accuracy (%):  1.0\n",
      "Iteration : 900  Loss:  0.0948036246598  Test Loss:  0.0361005556678 Accuracy (%):  1.0\n",
      "Iteration : 1000  Loss:  0.0919917876254  Test Loss:  0.0322619729069 Accuracy (%):  1.0\n",
      "Total training time (seconds):  0.40627074241638184\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(iters):\n",
    "    \n",
    "    #Fowardpropagation\n",
    "    inp, z, a, preds, J = Forwards(train[:,0:4],Theta1,Theta2,train[:,4],Neuron.ReLU,Regularisation=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    d2, d1 = Backwards(inp, z, a, preds, train[:,4], NeuronPrime.ReLUPrime, Regularisation=True)\n",
    "    \n",
    "    #SGD weight update\n",
    "    Theta1 = Theta1 - lr*d1\n",
    "    Theta2 = Theta2 - lr*d2\n",
    "    \n",
    "    #Fowardpropagate through test dataset\n",
    "    _,_,_,test_preds, J_test = Forwards(test[:,0:4],Theta1,Theta2,test[:,4],Neuron.ReLU,Regularisation=False)\n",
    "    \n",
    "    if (i + 1) % 100 == 0 or i == 0:\n",
    "        print(\"Iteration :\",i+1,\" Loss: \",J,\" Test Loss: \", J_test, \"Accuracy (%): \", Accuracy(test_preds,test[:,4]))\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Total training time (seconds): \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network below is broken. You need to fix the code to complete the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have repaired your neural network, then train the following model:\n",
    "\n",
    "1. Sigmoid neurons with regularisation.\n",
    "\n",
    "If you have correctly implemented your neural network, then the training loss, test loss and test accuracy at 1000 iterations should be 0.166583658885, 0.156340344383 and 0.966666666667.\n",
    "\n",
    "1. Tanh neurons with regularisation.\n",
    "2. ReLU neurons with regularisation.\n",
    "3. ReLU neurons with NO regularisation.\n",
    "\n",
    "Train each model for 1000 iterations and record the training set loss, the test set loss, accuracy and training time. \n",
    "1. Which is the best model by test loss? \n",
    "2. Which model has the highest accuracy?\n",
    "3. Of the two ReLU models, which has the lowest test loss? The model with regularisation or without it?\n",
    "4. Which model was the fastest to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you finish early and want an additional challenge, train a neural network using the \"softplus\" neuron, which has the form f(x) = log(1 + e^x). Fill in the appropriate function and derivative in the classes above. HINT - the derivative of the softplus neuron is another kind of neuron we've used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the initial theta values that you will use to intialise your models.\n",
    "\n",
    "Theta1_init = np.load(\"Theta1_ini.npy\")\n",
    "Theta2_init = np.load(\"Theta2_ini.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This passes the initial weight values to the theta weights your model will be training.\n",
    "\n",
    "Theta1 = Theta1_init\n",
    "Theta2 = Theta2_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training iterations\n",
    "\n",
    "iters = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(iters):\n",
    "    \n",
    "    #Fowardpropagation\n",
    "    \n",
    "    inp, z, a, preds, J = \n",
    "    \n",
    "    #Backpropagation\n",
    "    \n",
    "    \n",
    "    \n",
    "    #SGD weight update\n",
    "    Theta1 = Theta1 + lr*d1\n",
    "    Theta2 = Theta2 - lr*d2\n",
    "    \n",
    "    #Fowardpropagate through test dataset\n",
    "    _,_,_,test_preds, J_test = Forwards(test[:,0:4],Theta1,Theta2,test[:,4],Neuron.Tanh,False)\n",
    "    \n",
    "    if (i + 1) % 100 == 0 or i == 0:\n",
    "        print(\"Iteration :\",i+1,\" Loss: \",J,\" Test Loss: \", J_test, \"Accuracy (%): \", Accuracy(test_preds,test[:,4]))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total training time (seconds): \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
